---
layout: about
title: about
permalink: /
description: >
    <a href="https://lti.cmu.edu/people/students/xie-yiqing.html">Language Technologies Institute, CMU</a>. <a href="mailto:yiqingxi@andrew.cmu.edu?subject=Hi">yiqingxi@andrew.cmu.edu</a>
profile:
  align: right
  image: website_photo.jpg
  address: >
    <p>Rm 6607, Gates and Hillman Centers</p>
    <p>4902 Forbes Ave</p>
    <p>Pittsburgh, PA 15213, USA</p>

news: true  # includes a list of news items
sections: true
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I am a third-year Ph.D. student at the Language Technologies Institute of Carnegie Mellon University and I am working with [Carolyn Ros√©](https://cp3a.github.io/) and [Daniel Fried](https://dpfried.github.io){:target="\_blank"}. Previously, I obtained my Master degree in the data mining group at the University of Illinois Urbana-Champaign supervised by [Jiawei Han](http://hanj.cs.illinois.edu){:target="\_blank"} and obtained my Bachelor degree in Hong Kong University of Science and Technology, where I received the Academic Achievement Medal.

My research mainly focuses on annotation-efficient generation and evaluation systems, especially on code generation.
The topics including (i) building generalizable and annotation efficient NLP systems to assist human with practical tasks, and (ii) building reliable and automatic evaluation systems for NLP methods.
<!-- My research goal is to build generalizable, scalable and annotation efficient systems to assist human with practical tasks. This includes: -->

<!-- **I'm looking for a summer 2025 internship. Feel free to reach out if you have an opportunity that aligns!** -->

<!-- <p style="color: #0a5fd7; font-weight: bold;">I'm looking for a summer 2025 internship / collaboration on code generation or coding agents. Feel free to reach out if you have an opportunity that aligns!</p> -->

<br>


**Annotation-efficient NLP Systems**
  * Pretraining & continuous pretraining ([Anchor-DR](https://arxiv.org/abs/2305.05834), [METRO-T0](https://arxiv.org/abs/2305.12567))
  * Training environment ([RepoST](https://repost-code-gen.github.io/))
  * Model-generated Reward Signals ([FenCE](https://arxiv.org/abs/2410.18359))
  * Data augmentation ([FenCE](https://arxiv.org/abs/2410.18359), [Anchor-DR](https://arxiv.org/abs/2305.05834), [CMTrans](https://arxiv.org/abs/2311.00317), [Eider](https://arxiv.org/abs/2106.08657))
  * Guidance under heuristic metrics or prior knowledge ([AlaGCN](https://www.cs.emory.edu/~jyang71/files/alagnn.pdf), [RL-MMR](https://arxiv.org/abs/2010.00117), [KoMen](https://www.cs.emory.edu/~jyang71/files/komen.pdf))
  * Unsupervised or Semi-supervised methods ([Set-CoExpan](https://arxiv.org/abs/2001.10106), [CoRel](https://arxiv.org/abs/2010.06714))

**Reliable and Automatic Evaluation Systems**
  * Evaluation Benchmarks ([RepoST](https://repost-code-gen.github.io/), [TheAgentCompany](https://arxiv.org/abs/2412.14161), [CodeRAG-Bench](https://arxiv.org/abs/2406.14497), [CodeBenchGen](https://arxiv.org/abs/2404.00566))
  * Evaluation frameworks ([DocLens](https://arxiv.org/abs/2311.09581))
  * Evaluator models ([FenCE](https://arxiv.org/abs/2410.18359))

**LLMs for Code Generation**
  * Code generation training ([RepoST](https://repost-code-gen.github.io/), [CMTrans](https://arxiv.org/abs/2311.00317))
  * Code generation evaluation ([RepoST](https://repost-code-gen.github.io/), [TheAgentCompany](https://arxiv.org/abs/2412.14161), [CodeRAG-Bench](https://arxiv.org/abs/2406.14497), [CodeBenchGen](https://arxiv.org/abs/2404.00566))
  * Code generation analysis ([SACL](https://arxiv.org/abs/2506.20081), [Strong-Weak-colab](https://arxiv.org/abs/2505.20182))